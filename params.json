{"name":"Question Answering for Frequently Asked Questions (QA-FAQ)","tagline":"EVALITA 2016 - QA-FAQ: Question Answering for Frequently Asked Questions Task","body":"### Brief task description\r\n\r\nSearching within the Frequently Asked Questions (FAQ) page of a web site is a critical task: customers might feel overloaded by many irrelevant questions and become frustrated due to the difficulty in finding the FAQ suitable for their problems. Perhaps they are right there, but just worded in a different way than they know. \r\n\r\nThe proposed task consists in retrieving a list of relevant FAQs and corresponding answers related to the query issued by the user.\r\n\r\n[Acquedotto Pugliese (AQP)](http://www.aqp.it/) developed a semantic retrieval engine for FAQs, called [AQP Risponde](http://aqprisponde.aqp.it/ask.php), based on Question Answering (QA) techniques. The system allows customers to ask their own questions, and retrieves a list of relevant FAQs and corresponding answers. Furthermore, customers can select one FAQ among those retrieved by the system and can provide their feedback about the perceived accuracy of the answer.\r\n\r\nThis task poses relevant research challenges concerning both the usage of the Italian language in a deep QA architecture, and the variety of language expressions adopted by customers to formulate the same information need.\r\n\r\nThe task that we propose is strongly related to the recently organized task at Semeval 2015 and 2016 about Answer Selection in Community Question Answering [1]. This task helps to automate the process of finding good answers to new questions in a community-created discussion forum (e.g., by retrieving similar questions in the forum and by identifying the posts in the answer threads of similar questions that answer the original one as well).\r\nMoreover, the QA-FAQ has some common points with the Textual Similarity task [2] that received an increasing amount of attention in recent years.\r\n\r\n### Description and examples of the data\r\n\r\nAQP received about 24,000 questions and collected about 2,500 user feedbacks. We rely on these data for building the dataset for the task. In particular, we provide:\r\n\r\n+ a knowledge base of about 400 FAQs. Each FAQ is composed of a question, an answer, and a set of metadata (category and tags);\r\n+ a set of user queries. The queries will be collected by analyzing the AQP Risponde system log;\r\n+ a set of pairs <query, relevant faq> that will be exploited to evaluate the contestants. We build these pairs by analyzing the user feedbacks provided by the real users of AQP Risponde. We will manually check the user feedbacks in order to remove noisy or false feedbacks.\r\n\r\nWe will provide a little sample set for the system development and a test set for the evaluation. Should we collect a sufficient number of user feedbacks, we will also provide a set of training data. However, AQP is interested in the development of *unsupervised systems* since AQP Risponde should be able to provide good performance without any feedback.\r\n\r\nFollowing, an example of FAQ is reported:<br>\r\n> **Question:** Come posso telefonare al numero verde da un cellulare?<br>\r\n**Answer:** \"E' possibile chiamare il Contact Center AQP per segnalare un guasto o per un pronto intervento telefonando gratuitamente anche da cellulare al numero verde 800.735.735. Mentre per chiamare il Contact Center AQP per servizi commerciali 800.085.853 da un cellulare e dall'estero e necessario comporre il  numero +39.080.5723498 (il costo della chiamata e secondo il piano tariffario del chiamante).\"<br>\r\n**Category:** numero verde<br>\r\n**Tags:** canali, numero verde, cellulare\r\n\r\nThe previous FAQ is relevant for the query: \"si può telefonare da cellulare al numero verde?\". We will provide a simple baseline based on a classical Information Retrieval model.\r\n\r\n### References\r\n\r\n1 Preslav Nakov,  Lluís Márquez,  Walid Magdy,  Alessandro  Moschitti, Jim Glass,  and  Bilal Randeree. SemEval-2015 Task 3: Answer Selection in Community Question Answering.  Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), June 2015, Association for Computational Linguistics, 269-281, http://www.aclweb.org/anthology/S15-2047.\r\n\r\n2 Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria and Janyce Wiebe. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), June 2015, Association for Computational Linguistics, 252-263, http://www.aclweb.org/anthology/S15-2045.\r\n***\r\n### Task Organizers\r\n\r\n+ *Francesco Lovecchio*, AQP S.p.a.\r\n+ *Vito Manzari*, SudSistemi S.r.l.\r\n+ *Marco de Gemmis*, QuestionCube S.r.l.\r\n+ *Pasquale Lops*, Dipartimento di Informatica, Università degli Studi di Bari Aldo Moro\r\n+ *Annalina Caputo*, Dipartimento di Informatica, Università degli Studi di Bari Aldo Moro\r\n\r\n### Contact\r\n\r\nE-mail: <qa4faq@gmail.com>\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}